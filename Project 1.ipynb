{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Import only training datasets since kaggle's sorcing system does not function correctly\n",
    "2. Split training data sets into two parts: training & testing sets\n",
    "3. Feature engineering\n",
    "    * Since each sample covers 10 sensor channels and 128 measurements per time series, we need to group these measurements into one sample\n",
    "    * Measurements are extracted by grouping the series on functions: max, min, median, mean, std, absolute maximum and quantiles\n",
    "4. Use Random Forest Classifier (from scikit-learn) to train the model\n",
    "5. Check model accuracy\n",
    "    * OOB score\n",
    "    * 10-Fold cross validation\n",
    "    * 20 samples testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./Data/career-con-2019/X_train.csv')\n",
    "y_train = pd.read_csv('./Data/career-con-2019/y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the training set looks like  \n",
    "It has 487680 samples, each has 13 features (not all of them are usable and will be dropped later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.ndim, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split training sets into two parts\n",
    "* Last 20 groups will be used as testing set\n",
    "* Rest will be used as traning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X_train\n",
    "samples = 20\n",
    "time_series = 128\n",
    "start_x = X_train.shape[0] - samples*time_series\n",
    "X_train, X_test = X_train.iloc[:start_x], X_train.iloc[start_x:]\n",
    "# split y_train\n",
    "start_y = y_train.shape[0] - samples\n",
    "y_train, y_test = y_train.iloc[:start_y], y_train.iloc[start_y:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on to modify the data, first check out the information of the set  \n",
    "This can give us some insight of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's see how many series each type has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['surface'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process datasets\n",
    "The first thing we want to do is to drop those useless columns (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['row_id', 'measurement_number'], axis=1)\n",
    "X_test = X_test.drop(['row_id', 'measurement_number'], axis=1)\n",
    "y_train = y_train.drop('group_id', axis=1)\n",
    "y_test = y_test.drop('group_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 128 measurements for each series, it is hard to train  \n",
    "So we want to compress these 128 measurements into a single measurement  \n",
    "First, we will add 3 more features to each measurement, which will be the sum of orientation, angular_velocity, and linear_acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['orientation', 'angular_velocity', 'linear_acceleration']\n",
    "for i in columns:\n",
    "    if(i == 'orientation'):\n",
    "        X_train[i] = X_train[i+'_X'] + X_train[i+'_Y'] + X_train[i+'_Z'] + X_train[i+'_W']\n",
    "        X_test[i] = X_test[i+'_X'] + X_test[i+'_Y'] + X_test[i+'_Z'] + X_test[i+'_W']\n",
    "    else:\n",
    "        X_train[i] = X_train[i+'_X'] + X_train[i+'_Y'] + X_train[i+'_Z']\n",
    "        X_test[i] = X_test[i+'_X'] + X_test[i+'_Y'] + X_test[i+'_Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to calculate *max, min, mean, median, abs_max, std, quartile(25%), quartile(50%), quartile(75%)* for each series (128 measurements) to maintain as much information as possiable  \n",
    "* Every sample has 13 features before, each of the features will become 9 values after compression, so there will be 9 * 13 features for each sample after calculation\n",
    "* There was 485120 measurements before compression, and will become 485120 / 128 = 3790 samples after calsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "T_train = pd.DataFrame()\n",
    "T_test = pd.DataFrame()\n",
    "\n",
    "for i in X_train.columns[1:]:\n",
    "    T_train[i+'_max'] = X_train.groupby(by='series_id')[i].max()\n",
    "    T_test[i+'_max'] = X_test.groupby(by='series_id')[i].max()\n",
    "\n",
    "    T_train[i+'_min'] = X_train.groupby(by='series_id')[i].min()\n",
    "    T_test[i+'_min'] = X_test.groupby(by='series_id')[i].min()\n",
    "\n",
    "    T_train[i+'_mean'] = X_train.groupby(by='series_id')[i].mean()\n",
    "    T_test[i+'_mean'] = X_test.groupby(by='series_id')[i].mean()\n",
    "\n",
    "    T_train[i+'_median'] = X_train.groupby(by='series_id')[i].median()\n",
    "    T_test[i+'_median'] = X_test.groupby(by='series_id')[i].median()\n",
    "\n",
    "    T_train[i+'_quantile_25'] = X_train.groupby(by='series_id')[i].quantile(0.25)\n",
    "    T_test[i+'_quantile_25'] = X_test.groupby(by='series_id')[i].quantile(0.25)\n",
    "\n",
    "    T_train[i+'_quantile_50'] = X_train.groupby(by='series_id')[i].quantile(0.5)\n",
    "    T_test[i+'_quantile_50'] = X_test.groupby(by='series_id')[i].quantile(0.5)\n",
    "\n",
    "    T_train[i+'_quantile_75'] = X_train.groupby(by='series_id')[i].quantile(0.75)\n",
    "    T_test[i+'_quantile_75'] = X_test.groupby(by='series_id')[i].quantile(0.75)\n",
    "\n",
    "    T_train[i+'_abs_max'] = X_train.groupby(by='series_id')[i].apply(lambda x: np.max(np.abs(x)))\n",
    "    T_test[i+'_abs_max'] = X_test.groupby(by='series_id')[i].apply(lambda x: np.max(np.abs(x)))\n",
    "\n",
    "    T_train[i+'_std'] = X_train.groupby(by='series_id')[i].std()\n",
    "    T_test[i+'_std'] = X_test.groupby(by='series_id')[i].std()\n",
    "    \n",
    "X_train = T_train\n",
    "X_test = T_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test model\n",
    "For this project, I am using Random Forest Classifier from scikit-learn to train the model  \n",
    "Since this project is a classify problem, it is very suitable for using ensemble learning  \n",
    "* we'll set 300 estimators (after trying many times, I find this number will get the nearly best result and won't cost too much time)\n",
    "* Set a random seed (I set it to the course number of AI hhh)\n",
    "* The default bootstrap parameter is set to True, it means that we are using bagging. But since there're almost 30% of the samples will not be chosen in the training model, we will set oob_score to True, so that we can use those not chosen smaples to test the accuracy of our model\n",
    "* Set n_jobs to -1, we want to use every single CPU to train our model so that it won't take too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=300,\n",
    "                                random_state=6613,\n",
    "                                oob_score=True,\n",
    "                                n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train['surface'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, let's see the accuracy of our model, and we are using:  \n",
    "* Out-of-bag score\n",
    "* 10-Fold cross validation\n",
    "* Testing set split from training set\n",
    "\n",
    "We can see that this model can get about 89.6% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = cross_val_score(rf_clf, X_train, y_train['surface'], cv = 10)\n",
    "print('Accuracy: {:.2f} (+/- {:.2f})'.format(scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.score(X_test, y_test['surface'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
